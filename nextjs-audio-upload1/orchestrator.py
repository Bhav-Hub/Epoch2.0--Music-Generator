# -*- coding: utf-8 -*-
"""workflow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CuPWb_ACc7N8AKfjQXxZDqQOI1Q8pQ5l
"""

from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, START, END
import operator
from transformers import pipeline
from typing import TypedDict, Annotated
from pydantic import BaseModel, Field
import torchaudio
from collections import Counter
from statistics import mean, mode
from langgraph.constants import Send
from IPython.display import Image, display
from dotenv import load_dotenv
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from transformers import AutoProcessor, MusicgenForConditionalGeneration
import scipy
import torch

load_dotenv()

class Section(BaseModel):
    audio_file: str = Field(description="")
    # playlist_genre: str = Field(description="")
    # playlist_sub_genre: str = Field(description="")
    # danceability: float = Field(description="")
    # energy: float = Field(description="")
    # key: int = Field(description="")
    # loudness: float = Field(description="")
    # mode: int = Field(description="")
    # speechiness: float = Field(description="")
    # acousticness: float = Field(description="")
    # instrumentalness: float = Field(description="")
    # liveness: float = Field(description="")
    # valence: float = Field(description="")
    # tempo: float = Field(description="")

class Sections(BaseModel):
    sections: list[Section] = Field(description="A list of sections with audio file, energy, danceability, and tempo")

class State(TypedDict):
    sections: list[Section]

    compiled_audio: Annotated[list, operator.add]
    # compiled_playlist_genre: Annotated[list, operator.add]
    # compiled_playlist_sub_genre: Annotated[list, operator.add]
    # compiled_danceability: Annotated[list, operator.add]
    # compiled_energy: Annotated[list, operator.add]
    # compiled_key: Annotated[list, operator.add]
    # compiled_loudness: Annotated[list, operator.add]
    # compiled_mode: Annotated[list, operator.add]
    # compiled_speechiness: Annotated[list, operator.add]
    # compiled_acousticness: Annotated[list, operator.add]
    # compiled_instrumentalness: Annotated[list, operator.add]
    # compiled_liveness: Annotated[list, operator.add]
    # compiled_valence: Annotated[list, operator.add]
    # compiled_tempo: Annotated[list, operator.add]

    final_playlist_sub_genre: list[str]
    final_danceability: list[float]
    final_energy: list[float]
    final_key: list[int]
    final_loudness: list[float]
    final_mode: list[int]
    final_acousticness: list[float]
    final_instrumentalness: list[float]
    final_liveness: list[float]
    final_valence: list[float]
    final_tempo: list[float]

    prompt: str



class WorkerState(TypedDict):
    section: Section
    compiled_audio: Annotated[list, operator.add]

def orchestrator(state: State):
    print("orchestrator")
    print(state)

    return state

def llm_call(state: WorkerState):
    print("worker")
    model = "Bhaveen/epoch_musical_instruments_identification_2"
    MAX_LENGTH=48000
    RATE_HZ = 16000

    pipe = pipeline("audio-classification", model=model, device=0)

    audio,rate=torchaudio.load(state["section"]["audio_file"])
    transform=torchaudio.transforms.Resample(rate,RATE_HZ)
    audio=transform(audio).numpy().reshape(-1)[:MAX_LENGTH]
    results = pipe(audio)

    result = [result["label"] for result in results[:4]]

    print(result)

    return{
        "compiled_audio": [result]
    }

def compute_statistics(data_dict):
    results = {}
    for key, values in data_dict.items():
        if all(isinstance(v, float) for v in values):  # Compute mean for float lists
            results[key] = mean(values)
        elif all(isinstance(v, int) for v in values):  # Compute mode for int lists
            results[key] = mode(values)
        elif all(isinstance(v, str) for v in values):  # Compute most common string
            results[key] = Counter(values).most_common(1)[0][0] if values else None
    return results


def synthesizer(state: State):
    print("synthesizer")
    compiled_audio = state["compiled_audio"]
    print("Compiled", compiled_audio)
    flattened_audio = [item for sublist in compiled_audio for item in sublist]
    print("Flattened", flattened_audio)
    word_counts = Counter(flattened_audio)
    print("Word counts", word_counts)
    most_common_instruments = word_counts.most_common(3)
    print("Most common instruments", most_common_instruments)
    instruments = ",".join([instrument for instrument, count in most_common_instruments])

    print("Instruments", instruments)
    stats_data = compute_statistics({key:value for key, value in state.items() if key != "compiled_audio"})


    result_str = ", ".join(f"{key.replace('final_', '').replace('_', ' ').capitalize()} is {value}"
                       for key, value in stats_data.items())

    prompt = f"The features of the songs in playlist are {result_str} and the most common instruments are {instruments}"

    return {"prompt": prompt}

llm = ChatGroq(model="mixtral-8x7b-32768")

def prompt_generator(state: State):
    prompt = state["prompt"]

    response = llm.invoke([
        SystemMessage("You are a helpful music composer that generates prompts for a music generator. For a given input, you will  mention the instruments and convert all the corresponding float values into natural language. For example: If energy is greater than a threshold, you will say that it is a fast song. If acousticness is higher than a threshold, then there are more instruments involved rather than electric music. For tempo, give the value in BPM as it is. Limit the length of the prompt to 50 words. Be direct and concise. Donot add unnecessary numbers such as energy value or valence value."),
        HumanMessage(prompt)
    ])

    return {"prompt": response.content}

processor = AutoProcessor.from_pretrained("facebook/musicgen-small")
model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def music_generator(state: State):
    prompt = state["prompt"]
    print()
    print(prompt)
    print()

    inputs = processor(
    text=[prompt],
    padding=True,
    return_tensors="pt",
    )

    audio_values = model.generate(**inputs, max_new_tokens=256*3)#
    sampling_rate = model.config.audio_encoder.sampling_rate
    scipy.io.wavfile.write("server_musicgen_out_15.wav", rate=sampling_rate, data=audio_values[0, 0].numpy())

def assign_workers(state: State):

    return [Send("llm_call", {"section": s}) for s in state["sections"]]

builder = StateGraph(State)

builder.add_node("orchestrator", orchestrator)
builder.add_node("llm_call", llm_call)
builder.add_node("synthesizer", synthesizer)
builder.add_node("prompt_generator", prompt_generator)
builder.add_node("music_generator", music_generator)

builder.add_edge(START, "orchestrator")
builder.add_conditional_edges(
    "orchestrator",
    assign_workers,
    ["llm_call"]
)
builder.add_edge("llm_call", "synthesizer")
builder.add_edge("synthesizer", "prompt_generator")
builder.add_edge("prompt_generator", "music_generator")
builder.add_edge("music_generator", END)

graph = builder.compile()